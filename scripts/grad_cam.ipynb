{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'c:\\\\Users\\\\abbaj\\\\Desktop\\\\Research\\\\rcnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.data_loaders.data_loader import Data_Loader\n",
    "from src.models.faster_rcnn import Faster_RCNN\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from pytorch_grad_cam import EigenCAM\n",
    "from pytorch_grad_cam.ablation_layer import AblationLayerFasterRCNN\n",
    "from pytorch_grad_cam.utils.model_targets import FasterRCNNBoxScoreTarget\n",
    "from pytorch_grad_cam.utils.reshape_transforms import fasterrcnn_reshape_transform\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_accross_batch_and_channels, scale_cam_image\n",
    "\n",
    "from torchvision.transforms import ConvertImageDtype, ToTensor\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import glob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesPath = \"C:/Users/abbaj/Desktop/Research/IDCIAv2/images/\"\n",
    "annotationsPath = \"C:/Users/abbaj/Desktop/Research/IDCIAv2/bounding_boxes_faster_rcnn_larger/\"\n",
    "loadFile = \"c:/Users/abbaj/Desktop/Research/rcnn/src/weights/faster_rcnn/Weights(6,199)_31-03-24-20-19-26.pt\"\n",
    "\n",
    "data_loader = Data_Loader(\n",
    "    images_path=imagesPath,\n",
    "    annotations_path=annotationsPath,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "rcnn = Faster_RCNN()\n",
    "rcnn.load_model(model_path=loadFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_idx = 249\n",
    "\n",
    "rcnn.model.eval()\n",
    "image_files: list = glob2.glob(imagesPath + \"*.tiff\")\n",
    "image = ToTensor()(Image.open(image_files[image_idx]))\n",
    "outputs = rcnn.model([image])\n",
    "print(f\"Image: {image_files[image_idx]} loaded.\")\n",
    "input_tensor_img = ConvertImageDtype(dtype=torch.uint8)(image)\n",
    "box = draw_bounding_boxes(\n",
    "    input_tensor_img,\n",
    "    boxes=outputs[0][\"boxes\"],\n",
    "    width=3,\n",
    "    colors=[\"red\"] * len(outputs[0][\"boxes\"]),\n",
    ")\n",
    "im = to_pil_image(box.detach())\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(boxes, image):\n",
    "    for i, box in enumerate(boxes):\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            (255,0,0), 2\n",
    "        )\n",
    "        cv2.putText(image, 'DAPI', (int(box[0]), int(box[1] - 5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,0,0), 2,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes, classes, labels = [], [], []\n",
    "for index in range(len(outputs)):\n",
    "    boxes.append(outputs[index][\"boxes\"].detach().numpy())\n",
    "    classes.append(outputs[index][\"labels\"])\n",
    "    labels.append(outputs[index][\"labels\"])\n",
    "boxes = np.int32(boxes)[0,:,:]\n",
    "classes = classes[0].detach().numpy()\n",
    "labels = labels[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_img\n",
    "torch.unsqueeze(input_tensor_img, 0) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = [rcnn.model.backbone]\n",
    "targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "cam = EigenCAM(rcnn.model,\n",
    "               target_layers,\n",
    "               reshape_transform=fasterrcnn_reshape_transform)\n",
    "\n",
    "grayscale_cam = cam(torch.unsqueeze(input_tensor_img, 0) / 255, targets=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb(im):\n",
    "    w, h = im.shape\n",
    "    ret = np.empty((w, h, 3), dtype=np.uint8)\n",
    "    ret[:, :, 0] = im\n",
    "    ret[:, :, 1] = im\n",
    "    ret[:, :, 2] = im\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first image in the batch:\n",
    "# grayscale_cam = grayscale_cam[0, :]\n",
    "image_float_np = np.float32(input_tensor_img) / 255\n",
    "print(to_rgb(image_float_np[0,:,:]).shape)\n",
    "print(grayscale_cam[0,:,:].shape)\n",
    "cam_image = show_cam_on_image(to_rgb(image_float_np[0,:,:]), grayscale_cam[0,:,:], use_rgb=True)\n",
    "# And lets draw the boxes again:\n",
    "image_with_bounding_boxes = draw_boxes(boxes, cam_image)\n",
    "Image.fromarray(image_with_bounding_boxes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_counting_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
